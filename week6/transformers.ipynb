{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "his task focusses on implementing and fine-tuning vision transformers for image classification.\n",
    "\n",
    "Your task is to implement and fine-tune (or train from scratch) a Vision Transformer model for an image classification task based on the CIFAR-10 dataset (depending on version size is below 200 MB). The task is designed to help you gain experience in applying ViTs to real-world computer vision problems. Feel free to use helpers of your choice, e.g. the huggingface/transformers library, which provides pre-built Vision Transformer models. Then you can load the CIFAR-10 dataset using the torchvision.datasets module.\n",
    "\n",
    "Got lost looking for a starting point? Here are two related examples (not tested):\n",
    "\n",
    "example on colab\n",
    "example from github\n",
    "example on kaggle\n",
    "In the best case, you create your own. Note that you do not need to compute the attention yourself by hand for this exercise.\n",
    "\n",
    "a. Load the CIFAR-10 dataset: Preprocess the images by resizing them to the appropriate size for Vision Transformers (e.g., 224x224). Normalize the dataset using standard image normalization techniques, and describe what those are.\n",
    "\n",
    "b. Load a pre-trained Vision Transformer model: Use a pre-trained ViT model from the transformers library. For example, you can use ViTForImageClassification from huggingface/transformers. Initialize the model with pre-trained weights (such as google/vit-base-patch16-224).\n",
    "\n",
    "c. Fine-tune the model: Ideally you do not want to train all parameters. (You can if you can manage.) Freeze the lower layers of the Vision Transformer to leverage the pre-trained features. Fine-tune the top layers of the model for CIFAR-10 image classification (10 classes). Use a suitable optimizer (e.g., AdamW) and a learning rate scheduler (e.g., linear decay with warmup).\n",
    "\n",
    "d. Train the model: Train the model for a fixed number of epochs (e.g., 10 epochs) and monitor the validation accuracy. Ensure that the model outputs the classification accuracy for both the training and validation sets after each epoch.\n",
    "\n",
    "e. Evaluation: After training, evaluate the model on the test set and report the final accuracy. Plot the training and validation loss curves and the accuracy curves. Compare how well the model performs with and without pre-training.\n",
    "\n",
    "f. (OPTIONAL) Visualize the attention maps from the Vision Transformer for some test images. Discuss how the model attends to different parts of the image for classification."
   ],
   "id": "e732ec1a8968a40c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T21:50:58.893895Z",
     "start_time": "2025-10-03T21:50:58.442348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a\n",
    "# . Load the CIFAR-10 dataset: Preprocess the images by resizing them to the appropriate size for Vision Transformers (e.g., 224x224). Normalize the dataset using standard image normalization techniques, and describe what those are.\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Standard ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ViT input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Download CIFAR-10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n"
   ],
   "id": "dc51d0a52327ba20",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# a\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# . Load the CIFAR-10 dataset: Preprocess the images by resizing them to the appropriate size for Vision Transformers (e.g., 224x224). Normalize the dataset using standard image normalization techniques, and describe what those are.\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtransforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransforms\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:53:19.440744Z",
     "start_time": "2025-10-03T19:53:18.381224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# b\n",
    "#  Load a pre-trained Vision Transformer model: Use a pre-trained ViT model from the transformers library. For example, you can use ViTForImageClassification from huggingface/transformers. Initialize the model with pre-trained weights (such as google/vit-base-patch16-224).\n",
    "\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=10,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n"
   ],
   "id": "6cc143ad0bdd875d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T19:54:36.801365Z",
     "start_time": "2025-10-03T19:54:36.794955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# c Fine-tune the model: Ideally you do not want to train all parameters. (You can if you can manage.) Freeze the lower layers of the Vision Transformer to leverage the pre-trained features. Fine-tune the top layers of the model for CIFAR-10 image classification (10 classes). Use a suitable optimizer (e.g., AdamW) and a learning rate scheduler (e.g., linear decay with warmup).\n",
    "\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False  # freeze backbone\n",
    "\n"
   ],
   "id": "f1722a3aed524dd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-10-03T21:48:22.781114500Z",
     "start_time": "2025-10-03T21:26:55.376393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# d Train the model: Train the model for a fixed number of epochs (e.g., 10 epochs) and monitor the validation accuracy. Ensure that the model outputs the classification accuracy for both the training and validation sets after each epoch.\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "        outputs = model(inputs).logits\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc {train_acc:.2f}%, Val Acc {val_acc:.2f}%\")\n",
    "\n",
    "\n"
   ],
   "id": "d0732fa78ea33b66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# e  Evaluation: After training, evaluate the model on the test set and report the final accuracy. Plot the training and validation loss curves and the accuracy curves. Compare how well the model performs with and without pre-training.\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "        outputs = model(inputs).logits\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ],
   "id": "30f3838f297e49d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot loss/accuracy curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save train_loss, val_loss, train_acc, val_acc inside loop above\n",
    "# Then plot:\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b054dd2d0f648378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# f optional - attention visualization\n",
    "\n",
    "outputs = model(inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # list of attention maps per layer\n",
    "print(len(attentions), attentions[0].shape)"
   ],
   "id": "221f8ec806bc02dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

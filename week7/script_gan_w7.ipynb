{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "\n",
    "Note: Carefully revisit the reading material, and choose additional sources if needed.\n",
    "\n",
    "- a. Briefly explain why the Generator’s loss function only depends on the Discriminator's output for generated images. (Note: different representations.)\n",
    "\n",
    "- b. What is “mode collapse” in GANs? Provide an intuitive example of how it might appear when generating images, and explain one strategy (theoretical or architectural) that can mitigate it. Provide a reference (from the provided reading material or beyond).\n",
    "\n",
    "- c. What is the difference between Inception Score (IS) and Fréchet Inception Distance (FID)?\n",
    "\n",
    "- d. Choose one application of GANs, and discuss potential ethical issue(s) that could arise.\n",
    "\n",
    "\n",
    "\n",
    "potential answers:\n",
    "\n",
    "\n",
    "\n",
    "Task 1: Theory - GAN background\n",
    "a. Generator's Loss Function Dependency\n",
    "The generator's loss function only depends on the discriminator's output for generated images because:\n",
    "\n",
    "The generator's objective is to fool the discriminator into classifying fake images as real\n",
    "\n",
    "During generator training, we freeze the discriminator and only update generator parameters\n",
    "\n",
    "The loss is calculated as: L_G = -log(D(G(z))) where we want D(G(z)) → 1 (real)\n",
    "\n",
    "We don't use real images in generator loss because the generator doesn't need to learn from real data directly - it learns indirectly through the discriminator's feedback\n",
    "\n",
    "b. Mode Collapse\n",
    "Definition: Mode collapse occurs when the generator produces limited varieties of samples, capturing only a few modes of the data distribution while ignoring others.\n",
    "\n",
    "Intuitive Example: When generating human faces, a GAN suffering from mode collapse might only generate faces of middle-aged Caucasian males, ignoring other ethnicities, ages, and genders.\n",
    "\n",
    "Mitigation Strategy: Mini-batch Discrimination (proposed in Improved GANs by Salimans et al.)\n",
    "\n",
    "The discriminator looks at multiple samples in a batch to detect if the generator is producing similar outputs\n",
    "\n",
    "This allows the discriminator to penalize lack of diversity\n",
    "\n",
    "Reference: Salimans, T., et al. \"Improved techniques for training gans.\" NeurIPS 2016.\n",
    "\n",
    "c. IS vs FID Comparison\n",
    "Inception Score (IS):\n",
    "\n",
    "Measures quality and diversity using pre-trained Inception network\n",
    "\n",
    "High score when: high confidence in predictions (quality) and diverse predictions across samples (diversity)\n",
    "\n",
    "Only uses generated images\n",
    "\n",
    "Fréchet Inception Distance (FID):\n",
    "\n",
    "Compares statistics of real and generated images in feature space\n",
    "\n",
    "Lower FID = better quality and diversity\n",
    "\n",
    "Uses both real and generated images\n",
    "\n",
    "Generally considered more reliable than IS\n",
    "\n",
    "d. Ethical Issues in GAN Applications\n",
    "Application: Deepfake generation for entertainment/media\n",
    "\n",
    "Ethical Issues:\n",
    "\n",
    "Identity theft and impersonation: Creating convincing fake videos of public figures\n",
    "\n",
    "Non-consensual pornography: Generating explicit content using people's likeness without consent\n",
    "\n",
    "Misinformation: Creating fake news or events that never happened\n",
    "\n",
    "Legal evidence tampering: Potential to undermine trust in video evidence\n",
    "\n",
    "\n",
    "\n",
    "another potential answer\n",
    "\n",
    "\n",
    "🧠 Task 1 — Theory\n",
    "a. Why the Generator’s loss depends only on the Discriminator’s output for generated images\n",
    "\n",
    "The generator never sees real data; it only receives feedback through the discriminator.\n",
    "Formally,\n",
    "\n",
    "min\n",
    "⁡\n",
    "𝐺\n",
    "𝐸\n",
    "𝑧\n",
    "∼\n",
    "𝑝\n",
    "𝑧\n",
    "[\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "𝐷\n",
    "(\n",
    "𝐺\n",
    "(\n",
    "𝑧\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "G\n",
    "min\n",
    "\t​\n",
    "\n",
    "E\n",
    "z∼p\n",
    "z\n",
    "\t​\n",
    "\n",
    "\t​\n",
    "\n",
    "[log(1−D(G(z)))]\n",
    "\n",
    "The generator adjusts its parameters θᵍ so that D(G(z)) → 1 (i.e., fake → real).\n",
    "Hence the loss depends only on D(G(z)) — the discriminator’s evaluation of generated samples.\n",
    "All gradient information about realism flows through that scalar output; the generator’s job is simply to make D believe its samples are real.\n",
    "\n",
    "Alternative “non-saturating” form\n",
    "\n",
    "max\n",
    "⁡\n",
    "𝐺\n",
    "𝐸\n",
    "𝑧\n",
    "∼\n",
    "𝑝\n",
    "𝑧\n",
    "[\n",
    "log\n",
    "⁡\n",
    "𝐷\n",
    "(\n",
    "𝐺\n",
    "(\n",
    "𝑧\n",
    ")\n",
    ")\n",
    "]\n",
    "G\n",
    "max\n",
    "\t​\n",
    "\n",
    "E\n",
    "z∼p\n",
    "z\n",
    "\t​\n",
    "\n",
    "\t​\n",
    "\n",
    "[logD(G(z))]\n",
    "\n",
    "This avoids vanishing gradients early in training but still depends solely on D(G(z)).\n",
    "\n",
    "b. Mode Collapse\n",
    "\n",
    "Definition:\n",
    "Mode collapse happens when the generator produces limited diversity — e.g., it discovers a few patterns that reliably fool the discriminator and repeats them.\n",
    "\n",
    "Example:\n",
    "In CIFAR-10 training, instead of generating ten diverse object classes, G outputs only one convincing “car-like” image with small variations.\n",
    "\n",
    "Mitigation strategies\n",
    "\n",
    "Architectural: Use Minibatch Discrimination (Salimans et al., 2016) → D looks at feature diversity across a batch.\n",
    "\n",
    "Objective modification: Wasserstein GAN with Gradient Penalty (WGAN-GP; Gulrajani et al., 2017) stabilizes training and discourages collapse by providing smooth, meaningful gradients.\n",
    "\n",
    "Training trick: Label smoothing, noise injection, or unrolling the discriminator.\n",
    "\n",
    "Reference:\n",
    "Gulrajani et al., “Improved Training of Wasserstein GANs,” NIPS 2017.\n",
    "\n",
    "c. Inception Score (IS) vs Fréchet Inception Distance (FID)\n",
    "Metric\tWhat it measures\tComputation\tLimitations\n",
    "IS\tImage quality + diversity based on Inception-v3 classifier outputs\t( \\exp(\\mathbb{E}_x [ KL(p(y\tx)\n",
    "FID\tHow close generated images are to real ones in feature space\tComputes Fréchet distance between multivariate Gaussians fitted to Inception features of real vs fake images\tMore robust, penalizes mode collapse and bad diversity\n",
    "✅ FID < better ⇒ closer to real distribution.\n",
    "d. Application + Ethical Issues\n",
    "\n",
    "Example Application: Face generation / deepfakes\n",
    "\n",
    "Ethical concerns:\n",
    "\n",
    "Misinformation → fake videos or portraits used for fraud or defamation.\n",
    "\n",
    "Consent → use of people’s faces without permission.\n",
    "\n",
    "Bias → training on imbalanced datasets amplifies societal biases.\n",
    "\n",
    "Mitigations: Watermarking, detectable synthetic data tags, ethics review for dataset use.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2\n",
    "\n",
    "Training a GAN can be challenging. In this exercise we invite you to try it for yourself, and see if you can succeed. We provide a sample notebook, which you can use as a starting point. You probably need to download some data first.\n",
    "\n",
    "Please note: In the provided sample notebook script_gan.ipynb, we use CIFAR10. You are free to use another dataset, MNIST or celebA. You might find it helpful to read more about the pytorch dataloader.\n",
    "\n",
    "Note: Remember to change the data path in the notebook.\n",
    "\n",
    "- a. Complete the notebook by adding the implementation for the generator and discriminator.\n",
    "- b. Which parts of the notebook have to be changed if you want to use another dataset? How do you need to change them?\n",
    "- c. Please document your findings when executing the notebook. Did your training converge? How did the intermediate generated images look like?\n",
    "- d. Name several things which can be changed to receive a different result after training.\n",
    "- e. Choose something to change, e.g. setting, preprocessing or parameters, and run the notebook (at least) 2 more times with different configurations. Save your results. Did the training converge? How do the generated images look like? How do the three different models perform in comparison?\n",
    "Non-mandatory: Comment on your suggestions to change the code."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T22:11:12.606373Z",
     "start_time": "2025-10-18T22:11:10.503186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T22:11:13.622801Z",
     "start_time": "2025-10-18T22:11:13.611797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a simple generator and discriminator for CIFAR-10\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 8, 8)),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128, momentum=0.78),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, momentum=0.78),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),\n",
    "            nn.BatchNorm2d(64, momentum=0.82),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128, momentum=0.82),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, momentum=0.8),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 5 * 5, 1),  # Fixed: calculated correct dimensions\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T22:11:17.675798Z",
     "start_time": "2025-10-18T22:11:17.623994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add this verification code right after your model definitions\n",
    "def verify_model_dimensions():\n",
    "    print(\"=== VERIFYING MODEL DIMENSIONS ===\")\n",
    "\n",
    "    # Test Generator\n",
    "    latent_dim = 100\n",
    "    generator = Generator(latent_dim)\n",
    "\n",
    "    # Test with batch of 4 noise vectors\n",
    "    test_noise = torch.randn(4, latent_dim)\n",
    "    print(f\"Generator input shape: {test_noise.shape}\")\n",
    "\n",
    "    try:\n",
    "        generated_images = generator(test_noise)\n",
    "        print(f\"Generator output shape: {generated_images.shape}\")\n",
    "\n",
    "        # Check if output matches 3x32x32\n",
    "        expected_shape = (4, 3, 32, 32)\n",
    "        if generated_images.shape == expected_shape:\n",
    "            print(\"✅ Generator CORRECT: Output matches 3x32x32\")\n",
    "        else:\n",
    "            print(f\"❌ Generator INCORRECT: Expected {expected_shape}, got {generated_images.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generator ERROR: {e}\")\n",
    "\n",
    "    # Test Discriminator\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # Create a proper 32x32 test image\n",
    "    test_images = torch.randn(4, 3, 32, 32)\n",
    "    print(f\"\\nDiscriminator input shape: {test_images.shape}\")\n",
    "\n",
    "    try:\n",
    "        discriminator_output = discriminator(test_images)\n",
    "        print(f\"Discriminator output shape: {discriminator_output.shape}\")\n",
    "\n",
    "        # Check if input is 3x32x32 and output is single number per image\n",
    "        if test_images.shape == (4, 3, 32, 32) and discriminator_output.shape == (4, 1):\n",
    "            print(\"✅ Discriminator CORRECT: Input=3x32x32, Output=single number\")\n",
    "        else:\n",
    "            print(\"❌ Discriminator INCORRECT\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Discriminator ERROR: {e}\")\n",
    "\n",
    "# Run verification\n",
    "verify_model_dimensions()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFYING MODEL DIMENSIONS ===\n",
      "Generator input shape: torch.Size([4, 100])\n",
      "Generator output shape: torch.Size([4, 3, 32, 32])\n",
      "✅ Generator CORRECT: Output matches 3x32x32\n",
      "\n",
      "Discriminator input shape: torch.Size([4, 3, 32, 32])\n",
      "Discriminator output shape: torch.Size([4, 1])\n",
      "✅ Discriminator CORRECT: Input=3x32x32, Output=single number\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T10:29:01.955429Z",
     "start_time": "2025-10-18T10:28:44.441047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data loading and preprocessing (using CIFAR-10 dataset)\n",
    "\n",
    "# train on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "\n",
    "print(f'Train: {len(dataset)} samples')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 50000 samples\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T23:16:21.495614Z",
     "start_time": "2025-10-18T23:16:21.083088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the models\n",
    "\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "num_epochs = 10\n",
    "\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "\n",
    "\n",
    "# Define loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "# def train_gan(generator, discriminator, dataloader, num_epochs):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, data in enumerate(dataloader):\n",
    "#             real_images, _ = data\n",
    "#             batch_size = real_images.size(0)\n",
    "#             real_images = real_images.view(batch_size, -1)\n",
    "#             real_labels = torch.ones(batch_size, 1)\n",
    "#             fake_labels = torch.zeros(batch_size, 1)\n",
    "#\n",
    "#             # Train the discriminator\n",
    "#             optimizer_D.zero_grad()\n",
    "#             outputs = discriminator(real_images)\n",
    "#             d_loss_real = criterion(outputs, real_labels)\n",
    "#             d_loss_real.backward()\n",
    "#\n",
    "#             z = torch.randn(batch_size, 100)\n",
    "#             fake_images = generator(z)\n",
    "#             outputs = discriminator(fake_images.detach())\n",
    "#             d_loss_fake = criterion(outputs, fake_labels)\n",
    "#             d_loss_fake.backward()\n",
    "#             d_loss = d_loss_real + d_loss_fake\n",
    "#             optimizer_D.step()\n",
    "#\n",
    "#             # Train the generator\n",
    "#             optimizer_G.zero_grad()\n",
    "#             outputs = discriminator(fake_images)\n",
    "#             g_loss = criterion(outputs, real_labels)\n",
    "#             g_loss.backward()\n",
    "#             optimizer_G.step()\n",
    "#\n",
    "#             d_losses.append(d_loss.item())\n",
    "#             g_losses.append(g_loss.item())\n",
    "#\n",
    "#             if (i + 1) % 100 == 0:\n",
    "#                 print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "#\n",
    "#         # Generate and save a sample of fake images\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 z = torch.randn(32, 100)\n",
    "#                 fake_samples = generator(z)\n",
    "#                 vutils.save_image(fake_samples, f'fake_cifar_samples_epoch_{epoch+1}.png', normalize=True)\n",
    "#\n",
    "#         # Plot the loss curves\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.title(\"Generator and Discriminator Loss\")\n",
    "#         plt.plot(g_losses, label=\"G Loss\")\n",
    "#         plt.plot(d_losses, label=\"D Loss\")\n",
    "#         plt.xlabel(\"Iterations\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.legend()\n",
    "#         plt.savefig(f'loss_plot_epoch_{epoch+1}.png')\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            real_images, _ = data\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Move to same device as models\n",
    "            device = next(generator.parameters()).device\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "            # Train the discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            outputs_real = discriminator(real_images)\n",
    "            d_loss_real = criterion(outputs_real, real_labels)\n",
    "\n",
    "            # Fake images\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            fake_images = generator(z)\n",
    "            outputs_fake = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs_fake, fake_labels)\n",
    "\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the generator\n",
    "            optimizer_G.zero_grad()\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        fixed_z = torch.randn(32, latent_dim, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_fixed = generator(fixed_z)\n",
    "            vutils.save_image(fake_fixed, f'fixed_progress_epoch_{epoch+1:02d}.png', normalize=True, nrow=8)\n",
    "            z = torch.randn(32, latent_dim, device=device)\n",
    "            fake_samples = generator(z)\n",
    "            vutils.save_image(fake_samples, f'fake_samples_epoch_{epoch+1:02d}.png',\n",
    "                            normalize=True, nrow=8)\n",
    "            print(f'Saved samples for epoch {epoch+1}')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'g_losses': g_losses,\n",
    "                'd_losses': d_losses\n",
    "            }, f'checkpoint_epoch_{epoch+1:02d}.pt')\n",
    "            print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
    "\n",
    "        # Save loss arrays to CSV for later plotting\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            pd.DataFrame({'G_loss': g_losses, 'D_loss': d_losses}) \\\n",
    "                .to_csv(f'losses_epoch_{epoch+1:02d}.csv', index=False)\n",
    "        # Plot loss curves every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.title(f\"Generator and Discriminator Loss - Epoch {epoch+1}\")\n",
    "            plt.plot(g_losses, label=\"G Loss\")\n",
    "            plt.plot(d_losses, label=\"D Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f'loss_plot_epoch_{epoch+1:02d}.png')\n",
    "            plt.close()\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m beta2 = \u001B[32m0.999\u001B[39m\n\u001B[32m      7\u001B[39m num_epochs = \u001B[32m10\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m generator = \u001B[43mGenerator\u001B[49m(latent_dim)\n\u001B[32m     10\u001B[39m discriminator = Discriminator()\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGenerator parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(p.numel()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mgenerator.parameters())\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'Generator' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-10-18T21:50:52.959795300Z",
     "start_time": "2025-10-18T10:36:45.678721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "train_gan(generator, discriminator, dataloader, num_epochs=num_epochs)\n",
    "print(\"Training completed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/10], Batch [20/3125], D Loss: 0.6065, G Loss: 0.9074\n",
      "Epoch [1/10], Batch [40/3125], D Loss: 0.4467, G Loss: 1.5799\n",
      "Epoch [1/10], Batch [60/3125], D Loss: 0.6644, G Loss: 1.3866\n",
      "Epoch [1/10], Batch [80/3125], D Loss: 0.5484, G Loss: 1.6885\n",
      "Epoch [1/10], Batch [100/3125], D Loss: 0.4316, G Loss: 1.6188\n",
      "Epoch [1/10], Batch [120/3125], D Loss: 0.4619, G Loss: 1.4403\n",
      "Epoch [1/10], Batch [140/3125], D Loss: 0.4939, G Loss: 1.5904\n",
      "Epoch [1/10], Batch [160/3125], D Loss: 0.6754, G Loss: 1.3510\n",
      "Epoch [1/10], Batch [180/3125], D Loss: 0.6325, G Loss: 1.1295\n",
      "Epoch [1/10], Batch [200/3125], D Loss: 0.6710, G Loss: 0.9767\n",
      "Epoch [1/10], Batch [220/3125], D Loss: 0.6282, G Loss: 0.9186\n",
      "Epoch [1/10], Batch [240/3125], D Loss: 0.7062, G Loss: 0.8775\n",
      "Epoch [1/10], Batch [260/3125], D Loss: 0.5164, G Loss: 1.0150\n",
      "Epoch [1/10], Batch [280/3125], D Loss: 0.6876, G Loss: 1.0355\n",
      "Epoch [1/10], Batch [300/3125], D Loss: 0.4625, G Loss: 1.2416\n",
      "Epoch [1/10], Batch [320/3125], D Loss: 0.6264, G Loss: 1.5687\n",
      "Epoch [1/10], Batch [340/3125], D Loss: 0.4920, G Loss: 1.3244\n",
      "Epoch [1/10], Batch [360/3125], D Loss: 0.9332, G Loss: 0.8586\n",
      "Epoch [1/10], Batch [380/3125], D Loss: 0.5728, G Loss: 1.3009\n",
      "Epoch [1/10], Batch [400/3125], D Loss: 0.7346, G Loss: 1.0703\n",
      "Epoch [1/10], Batch [420/3125], D Loss: 0.4897, G Loss: 1.1165\n",
      "Epoch [1/10], Batch [440/3125], D Loss: 0.3245, G Loss: 1.5484\n",
      "Epoch [1/10], Batch [460/3125], D Loss: 0.7641, G Loss: 1.5948\n",
      "Epoch [1/10], Batch [480/3125], D Loss: 0.5578, G Loss: 1.6815\n",
      "Epoch [1/10], Batch [500/3125], D Loss: 0.8075, G Loss: 1.0481\n",
      "Epoch [1/10], Batch [520/3125], D Loss: 0.5429, G Loss: 1.4573\n",
      "Epoch [1/10], Batch [540/3125], D Loss: 0.4037, G Loss: 1.4567\n",
      "Epoch [1/10], Batch [560/3125], D Loss: 0.6846, G Loss: 1.7063\n",
      "Epoch [1/10], Batch [580/3125], D Loss: 0.3995, G Loss: 1.7731\n",
      "Epoch [1/10], Batch [600/3125], D Loss: 0.8780, G Loss: 1.0303\n",
      "Epoch [1/10], Batch [620/3125], D Loss: 0.6927, G Loss: 1.2042\n",
      "Epoch [1/10], Batch [640/3125], D Loss: 0.6232, G Loss: 1.3019\n",
      "Epoch [1/10], Batch [660/3125], D Loss: 0.6349, G Loss: 1.3846\n",
      "Epoch [1/10], Batch [680/3125], D Loss: 0.7049, G Loss: 1.1305\n",
      "Epoch [1/10], Batch [700/3125], D Loss: 0.6343, G Loss: 0.8564\n",
      "Epoch [1/10], Batch [720/3125], D Loss: 0.7367, G Loss: 1.0620\n",
      "Epoch [1/10], Batch [740/3125], D Loss: 0.9297, G Loss: 0.9130\n",
      "Epoch [1/10], Batch [760/3125], D Loss: 0.8070, G Loss: 0.6273\n",
      "Epoch [1/10], Batch [780/3125], D Loss: 0.7273, G Loss: 0.8186\n",
      "Epoch [1/10], Batch [800/3125], D Loss: 0.8778, G Loss: 0.6787\n",
      "Epoch [1/10], Batch [820/3125], D Loss: 0.6050, G Loss: 0.8301\n",
      "Epoch [1/10], Batch [840/3125], D Loss: 0.7264, G Loss: 0.8044\n",
      "Epoch [1/10], Batch [860/3125], D Loss: 0.6012, G Loss: 0.7666\n",
      "Epoch [1/10], Batch [880/3125], D Loss: 0.7604, G Loss: 0.7402\n",
      "Epoch [1/10], Batch [900/3125], D Loss: 0.7063, G Loss: 0.8233\n",
      "Epoch [1/10], Batch [920/3125], D Loss: 0.7228, G Loss: 0.6513\n",
      "Epoch [1/10], Batch [940/3125], D Loss: 0.6226, G Loss: 0.7461\n",
      "Epoch [1/10], Batch [960/3125], D Loss: 0.7792, G Loss: 0.8056\n",
      "Epoch [1/10], Batch [980/3125], D Loss: 0.7300, G Loss: 0.6534\n",
      "Epoch [1/10], Batch [1000/3125], D Loss: 0.5973, G Loss: 1.0649\n",
      "Epoch [1/10], Batch [1020/3125], D Loss: 0.5912, G Loss: 1.0307\n",
      "Epoch [1/10], Batch [1040/3125], D Loss: 0.7386, G Loss: 0.7954\n",
      "Epoch [1/10], Batch [1060/3125], D Loss: 0.6215, G Loss: 0.9032\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
